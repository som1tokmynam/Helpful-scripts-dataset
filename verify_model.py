import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import sys
import os

# ===================================================================================
# ---                           CONFIGURATION                                     ---
# ===================================================================================
# --- REQUIRED PATHS ---
base_model_path = "/media/administrator/oiseauxai1data/modelweights/M3.2-36B-Instruct"
merged_model_path = "/media/administrator/oiseauxai1data/modelweights/M3.2-36B-Animus-V8.1"

# --- PROMPT FOR FIDELITY TEST ---
# This is now a structured list of messages, which is the correct input
# for tokenizer.apply_chat_template.
test_messages = [
    {
        "role": "system",
        "content": (
            "You are a creative and helpful assistant who acts as a game master for a fantasy role-playing game. "
            "When you are asked to provide a list of options or choices for the player, you MUST enclose the entire list "
            "within <choices> and </choices> tags."
        )
    },
    {
        "role": "user",
        "content": (
            "The hero stands before a sealed, ancient door humming with magical energy. What are her three possible courses of action? "
            "Please provide these as choices."
        )
    }
]
# ===================================================================================

def main():
    print("Starting Comprehensive Model Verification Script (Chat Template Version)...")
    
    # --- Part 1: Structural Integrity Test (Loading) ---
    print("\n--- 1. STRUCTURAL INTEGRITY TEST (LOADING) ---")
    try:
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(merged_model_path)
        print("Tokenizer loaded successfully.")

        # --- CRUCIAL CHECK ---
        # Verify that the tokenizer actually has the chat template.
        if tokenizer.chat_template is None:
            print("\n[FATAL FAILURE] The tokenizer config does not contain a 'chat_template'.")
            print("Please ensure your merged model's 'tokenizer_config.json' includes the Jinja template.")
            sys.exit(1)
        print("Found a valid chat template in the tokenizer's configuration.")

        print("Loading model... (This may take a while and use a lot of RAM/VRAM)")
        model = AutoModelForCausalLM.from_pretrained(
            merged_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
        print("\n[SUCCESS] Model loaded successfully onto memory/GPU(s)!")
        print(f"Model is using device(s): {model.device}")

    except Exception as e:
        print(f"\n[FATAL FAILURE] An error occurred while loading the model. It is likely broken.")
        print(f"Details: {e}")
        print("\nDo not proceed to quantization.")
        sys.exit(1)

    # --- Part 2: Sanity Check (Basic Generation) ---
    print("\n--- 2. SANITY CHECK (BASIC GENERATION) ---")
    # This test remains the same as it doesn't rely on the special template.
    prompt_sanity = "Once upon a time,"
    inputs_sanity = tokenizer(prompt_sanity, return_tensors="pt").to(model.device)
    outputs_sanity = model.generate(**inputs_sanity, max_new_tokens=30)
    decoded_sanity = tokenizer.decode(outputs_sanity[0], skip_special_tokens=True)
    print(f"Sanity Prompt: '{prompt_sanity}'")
    print(f"Model Response: '{decoded_sanity}'")
    if len(decoded_sanity) <= len(prompt_sanity):
        print("[WARNING] Sanity check generated no new text. This could indicate a problem.")
    else:
        print("Sanity check passed.")

    # --- Part 3: Fidelity & Vocabulary Test (Using the Chat Template) ---
    print("\n--- 3. FIDELITY & VOCABULARY TEST ---")
    
    print("Applying chat template to the test messages...")
    # This is the key change: using apply_chat_template to build the prompt string.
    # tokenize=False gives us the string to inspect.
    # add_generation_prompt=True correctly formats it to prompt the assistant's turn.
    final_prompt_string = tokenizer.apply_chat_template(
        test_messages,
        tokenize=False,
        add_generation_prompt=True
    )
        
    print("\nFinal prompt string generated by tokenizer.apply_chat_template:")
    print("---------------------------------")
    print(final_prompt_string)
    print("---------------------------------")
    
    print("\nTokenizing the final prompt and generating a response...")
    inputs_fidelity = tokenizer(final_prompt_string, return_tensors="pt").to(model.device)
    outputs_fidelity = model.generate(**inputs_fidelity, max_new_tokens=250, do_sample=True, top_p=0.9, temperature=0.7)
    decoded_fidelity = tokenizer.decode(outputs_fidelity[0], skip_special_tokens=False)

    print("\nModel Fidelity Response:")
    print(decoded_fidelity)

    # Final conclusion
    print("\n--- Verification Complete ---")
    print("Review the outputs above.")
    print("- Did the model correctly generate text after the final `[/INST]` tag?")
    print("- Most importantly, did the response contain a list of choices enclosed in `<choices>` and `</choices>`?")
    print("If it did, you have a perfectly working model ready for quantization!")

if __name__ == "__main__":
    main()